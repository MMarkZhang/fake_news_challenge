{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance  \\\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated   \n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree   \n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated   \n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated   \n",
       "4  Spider burrowed through tourist's stomach and ...     1923   disagree   \n",
       "\n",
       "                                         articleBody  \n",
       "0  Danny Boyle is directing the untitled film\\r\\n...  \n",
       "1  Hundreds of Palestinians were evacuated from t...  \n",
       "2  30-year-old Moscow resident was hospitalized w...  \n",
       "3  (Reuters) - A Canadian soldier was shot at the...  \n",
       "4  Fear not arachnophobes, the story of Bunbury's...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies = pd.read_csv('train_bodies.csv')\n",
    "train = pd.read_csv('train_stances.csv')\n",
    "\n",
    "df_train = train.merge(train_bodies, on=['Body ID'], how='left')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance  \\\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated   \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated   \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated   \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated   \n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated   \n",
       "\n",
       "                                         articleBody  \n",
       "0  A RESPECTED senior French police officer inves...  \n",
       "1  Dave Morin's social networking company Path is...  \n",
       "2  A bereaved Afghan mother took revenge on the T...  \n",
       "3  Hewlett-Packard is officially splitting in two...  \n",
       "4  An airline passenger headed to Dallas was remo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bodies = pd.read_csv('competition_test_bodies.csv')\n",
    "test = pd.read_csv('competition_test_stances.csv')\n",
    "\n",
    "df_test = test.merge(test_bodies, on=['Body ID'], how='left')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a better way to split data into train and val (based on Body ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40350, 4)\n",
      "(9622, 4)\n",
      "(49972, 4)\n"
     ]
    }
   ],
   "source": [
    "# train_val_split = np.random.rand(max(df_train['Body ID'].tolist()) + 1)\n",
    "# ratio = 0.3\n",
    "# train_ids = []\n",
    "# val_ids = []\n",
    "# for idx, val in enumerate(train_val_split):\n",
    "#     if val < ratio:\n",
    "#         val_ids.append(idx)\n",
    "#     else:\n",
    "#         train_ids.append(idx)\n",
    "train_ids = [int(v) for v in open('training_ids.txt').read().strip().split('\\n')]\n",
    "val_ids = [int(v) for v in open('hold_out_ids.txt').read().strip().split('\\n')]\n",
    "        \n",
    "df_train_a = df_train[df_train['Body ID'].isin(train_ids)]\n",
    "df_train_b = df_train[df_train['Body ID'].isin(val_ids)]\n",
    "print(df_train_a.shape)\n",
    "print(df_train_b.shape)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a tfidf and serialize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function word_tokenize at 0x000001DB42A8F488>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize.word_tokenize, max_df=0.7, min_df=5, sublinear_tf=True)\n",
    "#tfidf = TfidfVectorizer(tokenizer=tokenize.word_tokenize, max_df=0.8, min_df=5, max_features=10000, sublinear_tf=True, ngram_range=(1,3))\n",
    "tfidf.fit(df_train['Headline'].tolist() + df_train['articleBody'].tolist() + df_test['Headline'].tolist() + df_test['articleBody'].tolist())\n",
    "joblib.dump(tfidf, 'tfidf.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanji\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf = joblib.load('tfidf.pkl')\n",
    "train_headline = tfidf.transform(df_train_a['Headline'].tolist())\n",
    "train_body = tfidf.transform(df_train_a['articleBody'].tolist())\n",
    "\n",
    "val_headline = tfidf.transform(df_train_b['Headline'].tolist())\n",
    "val_body = tfidf.transform(df_train_b['articleBody'].tolist())\n",
    "\n",
    "test_headline = tfidf.transform(df_test['Headline'].tolist())\n",
    "test_body = tfidf.transform(df_test['articleBody'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map y from string to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_2_id = {'agree': 0, 'disagree': 1, 'discuss':2, 'unrelated': 3}\n",
    "train_stance = df_train_a['Stance'].map(lambda x: label_2_id[x]).tolist()\n",
    "val_stance = df_train_b['Stance'].map(lambda x: label_2_id[x]).tolist()\n",
    "test_stance = df_test['Stance'].map(lambda x: label_2_id[x]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with the following features will generate a big gap between val score and test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = hstack((np.abs(train_headline - train_body), train_headline.multiply(train_body)))\n",
    "y_train = np.array(train_stance)\n",
    "\n",
    "X_val = hstack((np.abs(val_headline - val_body), val_headline.multiply(val_body)))\n",
    "y_val = np.array(val_stance)\n",
    "\n",
    "X_test = hstack((np.abs(test_headline - test_body), test_headline.multiply(test_body)))\n",
    "y_test = np.array(test_stance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcuate tfidf similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_sim_score(a, b):\n",
    "    assert a.shape == b.shape\n",
    "    res = []\n",
    "    for row in range(a.shape[0]):\n",
    "        cos = cosine_similarity(a[row], b[row])[0, 0]\n",
    "        res.append(cos)\n",
    "    return np.array(res).reshape(-1, 1)\n",
    "\n",
    "train_sim = get_sim_score(train_headline, train_body)\n",
    "val_sim = get_sim_score(val_headline, val_body)\n",
    "test_sim = get_sim_score(test_headline, test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31014083],\n",
       "       [0.02982301],\n",
       "       [0.01764494],\n",
       "       ...,\n",
       "       [0.00415531],\n",
       "       [0.48229947],\n",
       "       [0.00675724]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = hstack((train_headline, train_body, train_sim))\n",
    "y_train = np.array(train_stance)\n",
    "\n",
    "X_val = hstack((val_headline, val_body, val_sim))\n",
    "y_val = np.array(val_stance)\n",
    "\n",
    "X_test = hstack((test_headline, test_body, test_sim))\n",
    "y_test = np.array(test_stance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.63      0.68       762\n",
      "          1       0.65      0.36      0.47       162\n",
      "          2       0.83      0.85      0.84      1800\n",
      "          3       0.97      0.99      0.98      6898\n",
      "\n",
      "avg / total       0.92      0.92      0.92      9622\n",
      "\n",
      "acc: 0.9240282685512368\n",
      "macro f1: 0.7402175501165529\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print('classification report:')\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('acc:', accuracy_score(y_val, y_pred))\n",
    "print('macro f1:', f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.51      0.53      1903\n",
      "          1       0.50      0.01      0.01       697\n",
      "          2       0.77      0.74      0.75      4464\n",
      "          3       0.94      0.99      0.97     18349\n",
      "\n",
      "avg / total       0.87      0.88      0.87     25413\n",
      "\n",
      "--------------------------------------------------\n",
      "macro fscore: 0.564919\n",
      "--------------------------------------------------\n",
      "accuracy: 0.884744\n"
     ]
    }
   ],
   "source": [
    "final_clf = LinearSVC()\n",
    "final_clf.fit(X_train, y_train)\n",
    "y_pred = final_clf.predict(X_test)\n",
    "\n",
    "print('classification report: ')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('-' * 50)\n",
    "print('macro fscore: %.6f' % f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print('-' * 50)\n",
    "print('accuracy: %.6f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
