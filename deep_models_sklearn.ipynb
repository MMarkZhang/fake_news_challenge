{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk import tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import *\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance  \\\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated   \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated   \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated   \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated   \n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated   \n",
       "\n",
       "                                         articleBody  \n",
       "0  A RESPECTED senior French police officer inves...  \n",
       "1  Dave Morin's social networking company Path is...  \n",
       "2  A bereaved Afghan mother took revenge on the T...  \n",
       "3  Hewlett-Packard is officially splitting in two...  \n",
       "4  An airline passenger headed to Dallas was remo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies = pd.read_csv('./data/train_bodies.csv')\n",
    "train = pd.read_csv('./data/train_stances.csv')\n",
    "\n",
    "df_train = train.merge(train_bodies, on=['Body ID'], how='left')\n",
    "#df_train.head()\n",
    "\n",
    "test_bodies = pd.read_csv('./data/competition_test_bodies.csv')\n",
    "test = pd.read_csv('./data/competition_test_stances.csv')\n",
    "\n",
    "df_test = test.merge(test_bodies, on=['Body ID'], how='left')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40350, 4)\n",
      "(9622, 4)\n",
      "(49972, 4)\n"
     ]
    }
   ],
   "source": [
    "train_ids = [int(v) for v in open('./data/training_ids.txt').read().strip().split('\\n')]\n",
    "val_ids = [int(v) for v in open('./data/hold_out_ids.txt').read().strip().split('\\n')]\n",
    "        \n",
    "df_train_a = df_train[df_train['Body ID'].isin(train_ids)]\n",
    "df_train_b = df_train[df_train['Body ID'].isin(val_ids)]\n",
    "print(df_train_a.shape)\n",
    "print(df_train_b.shape)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec, vecs = load_word2vec('./data/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec_mean(sent, word2vec):\n",
    "    sent = [w.strip().lower() for w in tokenize.word_tokenize(sent)]\n",
    "    #print(len(sent))\n",
    "    sent = get_head_and_tail(sent)\n",
    "    #print(sent, len(sent))\n",
    "    vecs = np.array([word2vec[w] for w in sent if w in word2vec])\n",
    "    if len(vecs) > 0:\n",
    "        vecs = np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        vecs = np.array([0] * 100)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_headline_vec = [get_word2vec_mean(row, word2vec) for row in df_train_a['Headline'].tolist()]\n",
    "train_body_vec = [get_word2vec_mean(row, word2vec) for row in df_train_a['articleBody'].tolist()]\n",
    "\n",
    "val_headline_vec = [get_word2vec_mean(row, word2vec) for row in df_train_b['Headline'].tolist()]\n",
    "val_body_vec = [get_word2vec_mean(row, word2vec) for row in df_train_b['articleBody'].tolist()]\n",
    "\n",
    "test_headline_vec = [get_word2vec_mean(row, word2vec) for row in df_test['Headline'].tolist()]\n",
    "test_body_vec = [get_word2vec_mean(row, word2vec) for row in df_test['articleBody'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map y from string to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_2_id = {'agree': 0, 'disagree': 1, 'discuss':2, 'unrelated': 3}\n",
    "train_stance = df_train_a['Stance'].map(lambda x: label_2_id[x]).tolist()\n",
    "val_stance = df_train_b['Stance'].map(lambda x: label_2_id[x]).tolist()\n",
    "test_stance = df_test['Stance'].map(lambda x: label_2_id[x]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40350, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.concatenate((np.array(train_headline_vec), np.array(train_body_vec)), axis=1)\n",
    "y_train = np.array(train_stance)\n",
    "\n",
    "X_val = np.concatenate((np.array(val_headline_vec), np.array(val_body_vec)), axis=1)\n",
    "y_val = np.array(val_stance)\n",
    "\n",
    "X_test = np.concatenate((np.array(test_headline_vec), np.array(test_body_vec)), axis=1)\n",
    "y_test = np.array(test_stance)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89974985\n",
      "Iteration 2, loss = 0.65912192\n",
      "Iteration 3, loss = 0.54831656\n",
      "Iteration 4, loss = 0.47778836\n",
      "Iteration 5, loss = 0.43165866\n",
      "Iteration 6, loss = 0.39681980\n",
      "Iteration 7, loss = 0.36789052\n",
      "Iteration 8, loss = 0.35103896\n",
      "Iteration 9, loss = 0.33033510\n",
      "Iteration 10, loss = 0.31437489\n",
      "Iteration 11, loss = 0.30085063\n",
      "Iteration 12, loss = 0.28764652\n",
      "Iteration 13, loss = 0.27635357\n",
      "Iteration 14, loss = 0.26687505\n",
      "Iteration 15, loss = 0.25787818\n",
      "Iteration 16, loss = 0.24887153\n",
      "Iteration 17, loss = 0.24186295\n",
      "Iteration 18, loss = 0.23452927\n",
      "Iteration 19, loss = 0.22461413\n",
      "Iteration 20, loss = 0.21855029\n",
      "Iteration 21, loss = 0.21260348\n",
      "Iteration 22, loss = 0.20686145\n",
      "Iteration 23, loss = 0.20789744\n",
      "Iteration 24, loss = 0.19760742\n",
      "Iteration 25, loss = 0.19143424\n",
      "Iteration 26, loss = 0.18763388\n",
      "Iteration 27, loss = 0.18550603\n",
      "Iteration 28, loss = 0.17753694\n",
      "Iteration 29, loss = 0.17316150\n",
      "Iteration 30, loss = 0.17095040\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.53      0.51       762\n",
      "          1       0.17      0.04      0.07       162\n",
      "          2       0.73      0.80      0.76      1800\n",
      "          3       0.96      0.94      0.95      6898\n",
      "\n",
      "avg / total       0.86      0.87      0.86      9622\n",
      "\n",
      "accuracy: 0.867\n",
      "macro f1: 0.574\n",
      "score: 0.814\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 4), max_iter=30, random_state=42, verbose=True)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('accuracy: %.3f' % accuracy_score(y_val, y_pred))\n",
    "print('macro f1: %.3f' % f1_score(y_val, y_pred, average='macro'))\n",
    "print('score: %.3f' % (get_score(y_val, y_pred) / get_score(y_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.42      0.40      1903\n",
      "          1       0.17      0.02      0.03       697\n",
      "          2       0.55      0.56      0.56      4464\n",
      "          3       0.87      0.89      0.88     18349\n",
      "\n",
      "avg / total       0.76      0.77      0.77     25413\n",
      "\n",
      "accuracy: 0.774\n",
      "macro fscore: 0.468\n",
      "score: 0.666\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('macro fscore: %.3f' % f1_score(y_test, y_pred, average='macro'))\n",
    "print('score: %.3f' % (get_score(y_test, y_pred) / get_score(y_test, y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
